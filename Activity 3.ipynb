{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Exercise for Unit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a test sentence “The quick brown fox jumps over the lazy dog near the bank of the river.” OR generate your own test sentence, create a function that will determine the perplexity score for each trained model.\n",
    "\n",
    "### JEPHONE ISRAEL JIREH TORRE BSCS 3-A AI\n",
    "### CHERILYN MARIE DEOCAMPO BSCS 3-A AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\t (10 points) Use the Wikipedia python module and access any topic, as you will use that as your corpus, with a word limit of 1000 words.\n",
    "Example usage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\t(60 points) Train 2 models: a Bigram and Trigram Language Model, use the shared code as reference for bigram modeling, and update it to support trigrams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import wikipedia\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Wikipedia text (limited to 1000 words)\n",
    "text = wikipedia.summary(\"Natural language processing\", sentences=50)\n",
    "tokens = [word.lower() for word in word_tokenize(text) if word.isalnum()][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Unigram, Bigram, and Trigram counts\n",
    "unigrams = Counter(tokens)\n",
    "bigrams_c = Counter(bigrams(tokens))\n",
    "trigrams_c = Counter(trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size for Laplace Smoothing\n",
    "V = len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Probabilities with Smoothing\n",
    "bigram_probs = {bg: (count + 1) / (unigrams[bg[0]] + V) for bg, count in bigrams_c.items()}\n",
    "trigram_probs = {tg: (count + 1) / (bigrams_c.get((tg[0], tg[1]), 0) + V) for tg, count in trigrams_c.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Perplexity\n",
    "def compute_perplexity(test_sentence, model_type):\n",
    "    test_tokens = [word.lower() for word in word_tokenize(test_sentence) if word.isalnum()]\n",
    "    N = len(test_tokens)\n",
    "    \n",
    "    log_prob_sum = 0\n",
    "\n",
    "    if model_type == \"bigram\":\n",
    "        bigram_list = list(bigrams(test_tokens))\n",
    "        for bg in bigram_list:\n",
    "            prob = bigram_probs.get(bg, 1 / (unigrams.get(bg[0], 0) + V))  # Apply Laplace smoothing\n",
    "            log_prob_sum += math.log(prob)\n",
    "\n",
    "    elif model_type == \"trigram\":\n",
    "        trigram_list = list(trigrams(test_tokens))\n",
    "        for tg in trigram_list:\n",
    "            prob = trigram_probs.get(tg, 1 / (bigrams_c.get((tg[0], tg[1]), 0) + V))  # Apply Laplace smoothing\n",
    "            log_prob_sum += math.log(prob)\n",
    "\n",
    "    perplexity = math.exp(-log_prob_sum / N)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Test Sentence\n",
    "test_sentence = \"Artificial intelligence is transforming the way humans interact with technology.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity Scores\n",
    "bigram_pp = compute_perplexity(test_sentence, \"bigram\")\n",
    "trigram_pp = compute_perplexity(test_sentence, \"trigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\t(30 points) Using a test sentence “The quick brown fox jumps over the lazy dog near the bank of the river.” OR generate your own test sentence, create a function that will determine the perplexity score for each trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Perplexity -> Test Sentence: \"Artificial intelligence is transforming the way humans interact with technology.\" -> Score: 82.3791\n",
      "Trigram Model Perplexity -> Test Sentence: \"Artificial intelligence is transforming the way humans interact with technology.\" -> Score: 54.8439\n"
     ]
    }
   ],
   "source": [
    "# Display Results\n",
    "print(f\"Bigram Model Perplexity -> Test Sentence: \\\"{test_sentence}\\\" -> Score: {bigram_pp:.4f}\")\n",
    "print(f\"Trigram Model Perplexity -> Test Sentence: \\\"{test_sentence}\\\" -> Score: {trigram_pp:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
